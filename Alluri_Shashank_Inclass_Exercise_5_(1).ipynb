{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShashankAlluri28/INFO-5731Computational-Methods/blob/main/Alluri_Shashank_Inclass_Exercise_5_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 5**\n",
        "\n",
        "**This exercise aims to provide a comprehensive learning experience in text analysis and machine learning techniques, focusing on both text classification and clustering tasks.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks***.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission, and no requests will be answered. Manage your time accordingly.**\n"
      ],
      "metadata": {
        "id": "TU-pLW33lpcS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## **Question 1 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text classification** as well as the performance evaluation. In addition, you are requried to conduct **10 fold cross validation** (https://scikit-learn.org/stable/modules/cross_validation.html) in the training.\n",
        "\n",
        "\n",
        "\n",
        "The dataset can be download from canvas. The dataset contains two files train data and test data for sentiment analysis in IMDB review, it has two categories: 1 represents positive and 0 represents negative. You need to split the training data into training and validate data (80% for training and 20% for validation, https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6) and perform 10 fold cross validation while training the classifier. The final trained model was final evaluated on the test data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algorithms:**\n",
        "\n",
        "*   MultinominalNB\n",
        "*   SVM\n",
        "*   KNN\n",
        "*   Decision tree\n",
        "*   Random Forest\n",
        "*   XGBoost\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "**Evaluation measurement:**\n",
        "\n",
        "\n",
        "*   Accuracy\n",
        "*   Recall\n",
        "*   Precison\n",
        "*   F-1 score\n"
      ],
      "metadata": {
        "id": "loi8Sh7UE6ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the text file into a DataFrame\n",
        "with open('stsa-train.txt', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "data = []\n",
        "for line in lines:\n",
        "    sentiment, review = line.strip().split(' ', 1)\n",
        "    data.append([int(sentiment), review])\n",
        "\n",
        "df = pd.DataFrame(data, columns=['sentiment', 'review'])\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('train.csv', index=False)\n",
        "\n",
        "# Print the first few rows of the DataFrame\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GP1V_btPTPC0",
        "outputId": "69625657-847d-4b3d-9e04-c7831d3aa840"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   sentiment                                             review\n",
            "0          1  a stirring , funny and finally transporting re...\n",
            "1          0  apparently reassembled from the cutting-room f...\n",
            "2          0  they presume their audience wo n't sit still f...\n",
            "3          1  this is a visually stunning rumination on love...\n",
            "4          1  jonathan parker 's bartleby should have been t...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWRkrFJMjL6S",
        "outputId": "d1562ab0-a714-44c3-95a0-abcc8ce47d2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6920, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the text file into a DataFrame\n",
        "with open('stsa-test.txt', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "data = []\n",
        "for line in lines:\n",
        "    sentiment, review = line.strip().split(' ', 1)\n",
        "    data.append([int(sentiment), review])\n",
        "\n",
        "df1 = pd.DataFrame(data, columns=['sentiment', 'review'])\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df1.to_csv('test.csv', index=False)\n",
        "\n",
        "# Print the first few rows of the DataFrame\n",
        "print(df1.head())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIlXB3GOTLjM",
        "outputId": "913dc0a3-e470-48b1-ec0f-76cf8c40d241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   sentiment                                             review\n",
            "0          0     no movement , no yuks , not much of anything .\n",
            "1          0  a gob of drivel so sickly sweet , even the eag...\n",
            "2          0  gangs of new york is an unapologetic mess , wh...\n",
            "3          0  we never really feel involved with the story ,...\n",
            "4          1            this is one of polanski 's best films .\n",
            "(1821, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXgotemEjJJ_",
        "outputId": "7d637e23-9f44-405d-d51f-b834b1a60d77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1821, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the training and test data\n",
        "train_data = pd.read_csv('train.csv')\n",
        "test_data = pd.read_csv('test.csv')\n",
        "\n",
        "# Separate features (reviews) and labels (sentiments)\n",
        "X_train = train_data['review']\n",
        "y_train = train_data['sentiment']\n",
        "X_test = test_data['review']\n",
        "y_test = test_data['sentiment']\n",
        "\n",
        "# Split the training data into training and validation sets (80-20 split)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a dictionary of classifiers\n",
        "classifiers = {\n",
        "    \"MultinomialNB\": MultinomialNB(),\n",
        "    \"SVM\": SVC(),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"XGBoost\": XGBClassifier()\n",
        "}\n",
        "\n",
        "# Define evaluation metrics\n",
        "metrics = {\n",
        "    \"Accuracy\": accuracy_score,\n",
        "    \"Precision\": precision_score,\n",
        "    \"Recall\": recall_score,\n",
        "    \"F1 Score\": f1_score\n",
        "}\n",
        "\n",
        "# Perform 10-fold cross-validation on each classifier\n",
        "for name, clf in classifiers.items():\n",
        "    pipeline = make_pipeline(CountVectorizer(), clf)\n",
        "    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=10)\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(\"Cross-validation scores:\", cv_scores)\n",
        "    print(\"Mean Cross-validation score:\", cv_scores.mean())\n",
        "\n",
        "    # Train the final model\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate on the validation set\n",
        "    y_val_pred = pipeline.predict(X_val)\n",
        "    print(\"Validation Set Metrics:\")\n",
        "    for metric_name, metric_func in metrics.items():\n",
        "        metric_value = metric_func(y_val, y_val_pred)\n",
        "        print(f\"{metric_name}: {metric_value}\")\n",
        "\n",
        "# Evaluate the final trained models on the test data\n",
        "print(\"\\nFinal Evaluation on Test Data:\")\n",
        "for name, clf in classifiers.items():\n",
        "    pipeline = make_pipeline(CountVectorizer(), clf)\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    y_test_pred = pipeline.predict(X_test)\n",
        "    print(f\"\\n{name}:\")\n",
        "    for metric_name, metric_func in metrics.items():\n",
        "        metric_value = metric_func(y_test, y_test_pred)\n",
        "        print(f\"{metric_name}: {metric_value}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59Pw8Sv7aUgM",
        "outputId": "ae3a0dc1-9090-466c-c7a8-a3bfef5b60fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MultinomialNB:\n",
            "Cross-validation scores: [0.75631769 0.79783394 0.78519856 0.79061372 0.79061372 0.76173285\n",
            " 0.78481013 0.78481013 0.75406872 0.78481013]\n",
            "Mean Cross-validation score: 0.7790809565154947\n",
            "Validation Set Metrics:\n",
            "Accuracy: 0.7947976878612717\n",
            "Precision: 0.777490297542044\n",
            "Recall: 0.8429172510518934\n",
            "F1 Score: 0.8088829071332435\n",
            "\n",
            "SVM:\n",
            "Cross-validation scores: [0.73465704 0.72382671 0.71299639 0.72021661 0.74368231 0.76173285\n",
            " 0.7522604  0.72151899 0.71971067 0.75768535]\n",
            "Mean Cross-validation score: 0.734828732022901\n",
            "Validation Set Metrics:\n",
            "Accuracy: 0.7557803468208093\n",
            "Precision: 0.7394636015325671\n",
            "Recall: 0.8120617110799438\n",
            "F1 Score: 0.7740641711229947\n",
            "\n",
            "KNN:\n",
            "Cross-validation scores: [0.58483755 0.56137184 0.58844765 0.57581227 0.5631769  0.55234657\n",
            " 0.60036166 0.56600362 0.54972875 0.54068716]\n",
            "Mean Cross-validation score: 0.5682773973273447\n",
            "Validation Set Metrics:\n",
            "Accuracy: 0.6163294797687862\n",
            "Precision: 0.6226415094339622\n",
            "Recall: 0.6479663394109397\n",
            "F1 Score: 0.6350515463917525\n",
            "\n",
            "Decision Tree:\n",
            "Cross-validation scores: [0.63357401 0.60649819 0.63176895 0.61732852 0.64259928 0.63357401\n",
            " 0.64556962 0.64195298 0.6238698  0.61121157]\n",
            "Mean Cross-validation score: 0.628794693858899\n",
            "Validation Set Metrics:\n",
            "Accuracy: 0.6401734104046243\n",
            "Precision: 0.6405228758169934\n",
            "Recall: 0.6872370266479664\n",
            "F1 Score: 0.6630581867388362\n",
            "\n",
            "Random Forest:\n",
            "Cross-validation scores: [0.6967509  0.70216606 0.68772563 0.69314079 0.74729242 0.73285199\n",
            " 0.73598553 0.71971067 0.72151899 0.74141049]\n",
            "Mean Cross-validation score: 0.7178553475953284\n",
            "Validation Set Metrics:\n",
            "Accuracy: 0.7420520231213873\n",
            "Precision: 0.7160194174757282\n",
            "Recall: 0.8274894810659187\n",
            "F1 Score: 0.7677293428757319\n",
            "\n",
            "XGBoost:\n",
            "Cross-validation scores: [0.67870036 0.68772563 0.7166065  0.66967509 0.73465704 0.73104693\n",
            " 0.73056058 0.70524412 0.7124774  0.73417722]\n",
            "Mean Cross-validation score: 0.710087086518563\n",
            "Validation Set Metrics:\n",
            "Accuracy: 0.7398843930635838\n",
            "Precision: 0.7129071170084439\n",
            "Recall: 0.8288920056100981\n",
            "F1 Score: 0.7665369649805446\n",
            "\n",
            "Final Evaluation on Test Data:\n",
            "\n",
            "MultinomialNB:\n",
            "Accuracy: 0.8072487644151565\n",
            "Precision: 0.7852760736196319\n",
            "Recall: 0.8448844884488449\n",
            "F1 Score: 0.8139904610492847\n",
            "\n",
            "SVM:\n",
            "Accuracy: 0.757276221856123\n",
            "Precision: 0.7346733668341708\n",
            "Recall: 0.8041804180418042\n",
            "F1 Score: 0.7678571428571429\n",
            "\n",
            "KNN:\n",
            "Accuracy: 0.5831960461285008\n",
            "Precision: 0.5773195876288659\n",
            "Recall: 0.6160616061606161\n",
            "F1 Score: 0.5960617349654072\n",
            "\n",
            "Decision Tree:\n",
            "Accuracy: 0.6326194398682042\n",
            "Precision: 0.6239669421487604\n",
            "Recall: 0.6644664466446645\n",
            "F1 Score: 0.6435801811401172\n",
            "\n",
            "Random Forest:\n",
            "Accuracy: 0.7237781438769907\n",
            "Precision: 0.6994106090373281\n",
            "Recall: 0.7832783278327833\n",
            "F1 Score: 0.7389724961079398\n",
            "\n",
            "XGBoost:\n",
            "Accuracy: 0.727622185612301\n",
            "Precision: 0.6938967136150235\n",
            "Recall: 0.812981298129813\n",
            "F1 Score: 0.7487335359675785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. **MultinomialNB:**\n",
        "\n",
        "- obtained value the cross validation's result is the 0.779 .\n",
        "\n",
        "- According to the results indices of validation set, which I consider as a precise quality measurement a accuracy of 0.795 and a F1 score of 0.809 are promising.\n",
        "\n",
        "- Further analysis of test cases, a portion of the dataset that was not used to train and create the model, was excellent, and the classifier has an accuracy of 80.7% which is followed by the F1 score that reached 0.814.\n",
        "\n",
        "\n",
        "2. **SVM:**\n",
        "\n",
        "- The cross-validation accuracy shows that the model is truly effective and its value is 0.735.\n",
        "\n",
        "- The applied metrics showed that the set is in the average to good category, with an accuracy score of 75.6% and an F1 score of 77.4%.\n",
        "\n",
        "- The evaluation after that indicates that the accuracy rate at 0.757 and the F1 score at 0.768 are almost similar with a low performance reduction.\n",
        "\n",
        "\n",
        "3. **KNN:**\n",
        "\n",
        "- one of the confusion matrix entries is 0.568.\n",
        "\n",
        "- Rule of the Law is much more than not indiscriminate judgments direct. Inaccuracy ranges between 0.616 and 0.637, and F1-score is 0.637.\n",
        "\n",
        "- Looking at data to test, we had similar results to the comparative assessment that yielded the same 0.583 accuracy and 0.596 F1 score.\n",
        "\n",
        "\n",
        "4. **Decision Tree:**\n",
        "\n",
        "- CV accuracy is 0.629.\n",
        "\n",
        "- The validation dataset evaluation's output is 64.0% accuracy and 66.3% F1 score mediocre.\n",
        "\n",
        "- At last evaluation accuracy count of 0.633 is noted and f1-score= 0.64 for the test data as well.\n",
        "\n",
        "\n",
        "5. **Random Forest:**\n",
        "\n",
        "- k-fold cross-validation accuracy is 71.8 % for the cross validation model.\n",
        "\n",
        "- Validation is still good, and the accuracy is 74.2%, as well as the F1 score is 76.8%, this is quite a decent performance.\n",
        "\n",
        "- Evaluation reveals the model accuracy of 0.724 and the F1 score of 0.739, which is quite encouraging and meets our basic goals.\n",
        "\n",
        "6. **XGBoost:**\n",
        "\n",
        "- the cross validation accuracy is 0.710.\n",
        "\n",
        "- This is an indication of the validation set, and the accuracy is 0.740, with an F1 score of 0.767.\n",
        "\n",
        "- Moreover, the last testing on test data finally shows the exact performance level with the accuracy of 0.728 and the F1 score of 0.749.\n",
        "\n",
        "\n",
        "In general terms, MNB and XGBoost considerably proved their competence in the field. Finally, SVM and KNN were shown slightly lower efficacy. Random Forest also performs well. However, its F1-score for the test data is a bit less when compared with MultinomialNB and XGBoost Decision Tree produces good performance just like its counterpart in the case of valid data and test data."
      ],
      "metadata": {
        "id": "O09TqpW2emVl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## **Question 2 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text clustering**.\n",
        "\n",
        "Please downlad the dataset by using the following link.  https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
        "(You can also use different text data which you want)\n",
        "\n",
        "**Apply the listed clustering methods to the dataset:**\n",
        "*   K-means\n",
        "*   DBSCAN\n",
        "*   Hierarchical clustering\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "You can refer to of the codes from  the follwing link below.\n",
        "https://www.kaggle.com/karthik3890/text-clustering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence-transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Es2bo3cu2QoZ",
        "outputId": "b22b6aab-1727-4e48-b10e-6b2054cebb57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.40.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 sentence-transformers-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('train.csv')\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FV45XeyBIBOd",
        "outputId": "f982d4e4-9cfd-46ee-97b3-056c3865d1f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   sentiment                                             review\n",
            "0          1  a stirring , funny and finally transporting re...\n",
            "1          0  apparently reassembled from the cutting-room f...\n",
            "2          0  they presume their audience wo n't sit still f...\n",
            "3          1  this is a visually stunning rumination on love...\n",
            "4          1  jonathan parker 's bartleby should have been t...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**K-means**"
      ],
      "metadata": {
        "id": "ALU0dnE2aoA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Feature extraction using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X = vectorizer.fit_transform(data['review'])\n",
        "\n",
        "# Determine the optimal number of clusters using silhouette score\n",
        "max_clusters = 10\n",
        "best_score = -1\n",
        "best_k = 2\n",
        "for k in range(2, max_clusters + 1):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(X)\n",
        "    score = silhouette_score(X, kmeans.labels_)\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_k = k\n",
        "\n",
        "# Perform k-means clustering with the optimal number of clusters\n",
        "kmeans = KMeans(n_clusters=best_k, random_state=42)\n",
        "kmeans.fit(X)\n",
        "\n",
        "# Assign cluster labels to each review\n",
        "data['kmeans_cluster'] = kmeans.labels_\n",
        "\n",
        "# Output the number of clusters and points in each cluster\n",
        "num_clusters = len(set(kmeans.labels_))\n",
        "print(f\"Number of clusters found: {num_clusters}\")\n",
        "print(\"Number of points in each cluster:\")\n",
        "print(data['kmeans_cluster'].value_counts())\n",
        "\n",
        "# Output the reviews along with their assigned clusters\n",
        "print(\"\\nReviews with cluster labels:\")\n",
        "print(data[['review', 'kmeans_cluster']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wmyq10c2IOkr",
        "outputId": "0c5cacfd-f8dd-4654-fe5d-fc688490e419"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of clusters found: 10\n",
            "Number of points in each cluster:\n",
            "kmeans_cluster\n",
            "4    2037\n",
            "8    1200\n",
            "0     799\n",
            "2     756\n",
            "3     583\n",
            "7     428\n",
            "1     382\n",
            "9     305\n",
            "5     251\n",
            "6     179\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Reviews with cluster labels:\n",
            "                                                 review  kmeans_cluster\n",
            "0     a stirring , funny and finally transporting re...               4\n",
            "1     apparently reassembled from the cutting-room f...               8\n",
            "2     they presume their audience wo n't sit still f...               4\n",
            "3     this is a visually stunning rumination on love...               2\n",
            "4     jonathan parker 's bartleby should have been t...               8\n",
            "...                                                 ...             ...\n",
            "6915  painful , horrifying and oppressively tragic ,...               4\n",
            "6916  take care is nicely performed by a quintet of ...               0\n",
            "6917  the script covers huge , heavy topics in a bla...               4\n",
            "6918  a seriously bad film with seriously warped log...               3\n",
            "6919  a deliciously nonsensical comedy about a city ...               4\n",
            "\n",
            "[6920 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Output two reviews for each cluster\n",
        "print(\"Reviews with cluster labels:\")\n",
        "for cluster_label in range(best_k):\n",
        "    cluster_reviews = data[data['kmeans_cluster'] == cluster_label]['review'].head(2)\n",
        "    print(f\"\\nCluster {cluster_label}:\")\n",
        "    for review in cluster_reviews:\n",
        "        print(review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwgTT61_ZkH6",
        "outputId": "04022f64-0d28-4719-c39b-21838c0c8ec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reviews with cluster labels:\n",
            "\n",
            "Cluster 0:\n",
            "may be more genial than ingenious , but it gets the job done .\n",
            "if the tuxedo actually were a suit , it would fit chan like a $ 99 bargain-basement special .\n",
            "\n",
            "Cluster 1:\n",
            "final verdict : you 've seen it all before .\n",
            "you ... get a sense of good intentions derailed by a failure to seek and strike just the right tone .\n",
            "\n",
            "Cluster 2:\n",
            "this is a visually stunning rumination on love , memory , history and the war between art and commerce .\n",
            "the film is strictly routine .\n",
            "\n",
            "Cluster 3:\n",
            "a fan film that for the uninitiated plays better on video with the sound turned down .\n",
            "a little less extreme than in the past , with longer exposition sequences between them , and with fewer gags to break the tedium .\n",
            "\n",
            "Cluster 4:\n",
            "a stirring , funny and finally transporting re-imagining of beauty and the beast and 1930s horror films\n",
            "they presume their audience wo n't sit still for a sociology lesson , however entertainingly presented , so they trot out the conventional science-fiction elements of bug-eyed monsters and futuristic women in skimpy clothes .\n",
            "\n",
            "Cluster 5:\n",
            "while american adobo has its heart -lrb- and its palate -rrb- in the right place , its brain is a little scattered -- ditsy , even .\n",
            "-lrb- the kid 's -rrb- just too bratty for sympathy , and as the film grows to its finale , his little changes ring hollow .\n",
            "\n",
            "Cluster 6:\n",
            "blue crush follows the formula , but throws in too many conflicts to keep the story compelling .\n",
            "too bad .\n",
            "\n",
            "Cluster 7:\n",
            "for something as splendid-looking as this particular film , the viewer expects something special but instead gets -lrb- sci-fi -rrb- rehash .\n",
            "as quiet , patient and tenacious as mr. lopez himself , who approaches his difficult , endless work with remarkable serenity and discipline .\n",
            "\n",
            "Cluster 8:\n",
            "apparently reassembled from the cutting-room floor of any given daytime soap .\n",
            "jonathan parker 's bartleby should have been the be-all-end-all of the modern-office anomie films .\n",
            "\n",
            "Cluster 9:\n",
            "béart and berling are both superb , while huppert ... is magnificent .\n",
            "unfolds with such a wallop of you-are-there immediacy that when the bullets start to fly , your first instinct is to duck .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DBSCAN**"
      ],
      "metadata": {
        "id": "uM-zJeL9a0aA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Determine the optimal epsilon value using silhouette score\n",
        "best_eps = 0\n",
        "best_score = -1\n",
        "for eps in [0.1, 0.5, 1.0, 1.5, 2.0]:\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=5)\n",
        "    dbscan.fit(X)\n",
        "    unique_labels = len(set(dbscan.labels_)) - (1 if -1 in dbscan.labels_ else 0)  # Exclude noise points (-1)\n",
        "    if unique_labels > 1:  # Check if more than one cluster label is present\n",
        "        score = silhouette_score(X, dbscan.labels_)\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_eps = eps\n",
        "\n",
        "# Perform DBSCAN clustering with the optimal epsilon value\n",
        "dbscan = DBSCAN(eps=best_eps, min_samples=5)\n",
        "dbscan.fit(X)\n",
        "\n",
        "# Assign cluster labels to each review\n",
        "data['dbscan_cluster'] = dbscan.labels_\n",
        "\n",
        "# Output the number of clusters and points in each cluster\n",
        "num_clusters = len(set(dbscan.labels_)) - (1 if -1 in dbscan.labels_ else 0)  # Exclude noise points (-1)\n",
        "print(f\"Number of clusters found: {num_clusters}\")\n",
        "print(\"Number of points in each cluster:\")\n",
        "print(data['dbscan_cluster'].value_counts())\n",
        "\n",
        "# Output the reviews along with their assigned clusters\n",
        "print(\"\\nReviews with cluster labels:\")\n",
        "print(data[['review', 'dbscan_cluster']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2vaWvtGIUuC",
        "outputId": "b5198bd9-6e3c-4a67-a91c-c17664b1c514"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of clusters found: 4\n",
            "Number of points in each cluster:\n",
            "dbscan_cluster\n",
            "-1    6837\n",
            " 0      57\n",
            " 2      14\n",
            " 3       7\n",
            " 1       5\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Reviews with cluster labels:\n",
            "                                                 review  dbscan_cluster\n",
            "0     a stirring , funny and finally transporting re...              -1\n",
            "1     apparently reassembled from the cutting-room f...              -1\n",
            "2     they presume their audience wo n't sit still f...              -1\n",
            "3     this is a visually stunning rumination on love...              -1\n",
            "4     jonathan parker 's bartleby should have been t...              -1\n",
            "...                                                 ...             ...\n",
            "6915  painful , horrifying and oppressively tragic ,...              -1\n",
            "6916  take care is nicely performed by a quintet of ...              -1\n",
            "6917  the script covers huge , heavy topics in a bla...              -1\n",
            "6918  a seriously bad film with seriously warped log...              -1\n",
            "6919  a deliciously nonsensical comedy about a city ...              -1\n",
            "\n",
            "[6920 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Output two reviews for each cluster\n",
        "print(\"\\nTwo reviews for each cluster:\")\n",
        "for cluster_label in sorted(set(dbscan.labels_)):\n",
        "    if cluster_label == -1:\n",
        "        continue  # Skip noise points\n",
        "    cluster_reviews = data[data['dbscan_cluster'] == cluster_label]['review'].head(2)\n",
        "    print(f\"Cluster {cluster_label}:\")\n",
        "    for review in cluster_reviews:\n",
        "        print(f\"  - {review}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsMRnQ7uYU21",
        "outputId": "cd99297c-d4b5-4e39-e214-cff33f098545"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Two reviews for each cluster:\n",
            "Cluster 0:\n",
            "  - amazingly lame .\n",
            "  - wow .\n",
            "Cluster 1:\n",
            "  - fun and nimble .\n",
            "  - delirious fun .\n",
            "Cluster 2:\n",
            "  - technically and artistically inept .\n",
            "  - fierce , glaring and unforgettable .\n",
            "Cluster 3:\n",
            "  - remember it .\n",
            "  - it 's uninteresting .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hierarchical clustering**"
      ],
      "metadata": {
        "id": "vHgudxJka9wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Perform hierarchical clustering\n",
        "hierarchical = AgglomerativeClustering(n_clusters=2)\n",
        "hierarchical.fit(X.toarray())\n",
        "\n",
        "# Assign cluster labels to each review\n",
        "data['hierarchical_cluster'] = hierarchical.labels_\n",
        "\n",
        "# Output the reviews along with their assigned clusters\n",
        "print(\"Reviews with cluster labels:\")\n",
        "print(data[['review', 'hierarchical_cluster']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tE1qC-UO4Uz",
        "outputId": "d4df61ac-2a04-4c68-e777-d39967fd1bd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reviews with cluster labels:\n",
            "                                                 review  hierarchical_cluster\n",
            "0     a stirring , funny and finally transporting re...                     0\n",
            "1     apparently reassembled from the cutting-room f...                     1\n",
            "2     they presume their audience wo n't sit still f...                     0\n",
            "3     this is a visually stunning rumination on love...                     0\n",
            "4     jonathan parker 's bartleby should have been t...                     0\n",
            "...                                                 ...                   ...\n",
            "6915  painful , horrifying and oppressively tragic ,...                     0\n",
            "6916  take care is nicely performed by a quintet of ...                     0\n",
            "6917  the script covers huge , heavy topics in a bla...                     0\n",
            "6918  a seriously bad film with seriously warped log...                     0\n",
            "6919  a deliciously nonsensical comedy about a city ...                     0\n",
            "\n",
            "[6920 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the data by hierarchical cluster labels\n",
        "cluster_groups = data.groupby('hierarchical_cluster')\n",
        "\n",
        "# Print two reviews for each cluster\n",
        "for cluster_label, reviews_group in cluster_groups:\n",
        "    print(f\"Cluster {cluster_label}:\")\n",
        "    reviews = reviews_group['review'].head(2)  # Select two reviews from each cluster\n",
        "    for review in reviews:\n",
        "        print(review)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UtlwCK0Y-lj",
        "outputId": "9da9eb45-42e3-4044-e414-ba4bdc3e7e03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster 0:\n",
            "a stirring , funny and finally transporting re-imagining of beauty and the beast and 1930s horror films\n",
            "they presume their audience wo n't sit still for a sociology lesson , however entertainingly presented , so they trot out the conventional science-fiction elements of bug-eyed monsters and futuristic women in skimpy clothes .\n",
            "\n",
            "Cluster 1:\n",
            "apparently reassembled from the cutting-room floor of any given daytime soap .\n",
            "it is as uncompromising as it is nonjudgmental , and makes clear that a prostitute can be as lonely and needy as any of the clients .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word2Vec**"
      ],
      "metadata": {
        "id": "SzKYvTcHbGQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "\n",
        "# Train Word2Vec model\n",
        "sentences = [review.split() for review in data['review']]\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Generate embeddings for each review\n",
        "word2vec_features = []\n",
        "for review in sentences:\n",
        "    embeddings = [word2vec_model.wv[word] for word in review if word in word2vec_model.wv]\n",
        "    if embeddings:\n",
        "        review_embedding = sum(embeddings) / len(embeddings)\n",
        "        word2vec_features.append(review_embedding)\n",
        "    else:\n",
        "        word2vec_features.append([0] * 100)  # Handle case where review has no embeddings\n",
        "\n",
        "# Determine the optimal number of clusters for K-means clustering\n",
        "max_clusters = 10  # Maximum number of clusters to consider\n",
        "best_score = -1\n",
        "best_n_clusters = 2\n",
        "for n_clusters in range(2, max_clusters + 1):\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    kmeans.fit(word2vec_features)\n",
        "    score = silhouette_score(word2vec_features, kmeans.labels_)\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_n_clusters = n_clusters\n",
        "\n",
        "# Perform K-means clustering with the optimal number of clusters\n",
        "kmeans = KMeans(n_clusters=best_n_clusters, random_state=42)\n",
        "kmeans.fit(word2vec_features)\n",
        "data['word2vec_cluster'] = kmeans.labels_\n",
        "\n",
        "# Print the number of points in each cluster\n",
        "print(\"Number of points in each cluster:\")\n",
        "print(data['word2vec_cluster'].value_counts())\n",
        "\n",
        "# Print the output\n",
        "print(\"\\nWord2Vec clustering:\")\n",
        "print(data[['review', 'word2vec_cluster']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vY0Ww3oBIjNF",
        "outputId": "32f9d321-d058-4fc4-db15-b9698020c805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of points in each cluster:\n",
            "word2vec_cluster\n",
            "1    3638\n",
            "0    3282\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Word2Vec clustering:\n",
            "                                                 review  word2vec_cluster\n",
            "0     a stirring , funny and finally transporting re...                 0\n",
            "1     apparently reassembled from the cutting-room f...                 0\n",
            "2     they presume their audience wo n't sit still f...                 0\n",
            "3     this is a visually stunning rumination on love...                 1\n",
            "4     jonathan parker 's bartleby should have been t...                 0\n",
            "...                                                 ...               ...\n",
            "6915  painful , horrifying and oppressively tragic ,...                 1\n",
            "6916  take care is nicely performed by a quintet of ...                 0\n",
            "6917  the script covers huge , heavy topics in a bla...                 1\n",
            "6918  a seriously bad film with seriously warped log...                 0\n",
            "6919  a deliciously nonsensical comedy about a city ...                 0\n",
            "\n",
            "[6920 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print two reviews for each Word2Vec cluster\n",
        "print(\"\\nTwo reviews from each Word2Vec cluster:\")\n",
        "for cluster_id in range(best_n_clusters):\n",
        "    cluster_reviews = data[data['word2vec_cluster'] == cluster_id]['review'][:2]  # Get the first two reviews\n",
        "    print(f\"\\nCluster {cluster_id}:\\n\")\n",
        "    for review in cluster_reviews:\n",
        "        print(review)\n",
        "        print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4tUvpoBR5KL",
        "outputId": "2c17cbd4-a9e2-41a3-e91c-1e607760ca36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Two reviews from each Word2Vec cluster:\n",
            "\n",
            "Cluster 0:\n",
            "\n",
            "a stirring , funny and finally transporting re-imagining of beauty and the beast and 1930s horror films\n",
            "\n",
            "apparently reassembled from the cutting-room floor of any given daytime soap .\n",
            "\n",
            "\n",
            "Cluster 1:\n",
            "\n",
            "this is a visually stunning rumination on love , memory , history and the war between art and commerce .\n",
            "\n",
            "campanella gets the tone just right -- funny in the middle of sad in the middle of hopeful .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BERT**"
      ],
      "metadata": {
        "id": "6J-1ODcFbMzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Load pre-trained BERT model\n",
        "bert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "# Encode text reviews using BERT\n",
        "review_embeddings = bert_model.encode(data['review'].tolist())\n",
        "\n",
        "# Determine the optimal number of clusters for K-means clustering\n",
        "max_clusters = 10  # Maximum number of clusters to consider\n",
        "best_score = -1\n",
        "best_n_clusters = 2\n",
        "for n_clusters in range(2, max_clusters + 1):\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    kmeans.fit(review_embeddings)\n",
        "    score = silhouette_score(review_embeddings, kmeans.labels_)\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_n_clusters = n_clusters\n",
        "\n",
        "# Perform K-means clustering with the optimal number of clusters\n",
        "kmeans = KMeans(n_clusters=best_n_clusters, random_state=42)\n",
        "kmeans.fit(review_embeddings)\n",
        "data['bert_cluster'] = kmeans.labels_\n",
        "\n",
        "# Print the number of points in each cluster\n",
        "print(\"Number of points in each cluster:\")\n",
        "print(data['bert_cluster'].value_counts())\n",
        "\n",
        "# Print the output\n",
        "print(\"\\nBERT clustering:\")\n",
        "print(data[['review', 'bert_cluster']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VndoOzKNIlMm",
        "outputId": "82160b64-a5d7-4ef0-b10a-906a536411e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of points in each cluster:\n",
            "bert_cluster\n",
            "1    3905\n",
            "0    3015\n",
            "Name: count, dtype: int64\n",
            "\n",
            "BERT clustering:\n",
            "                                                 review  bert_cluster\n",
            "0     a stirring , funny and finally transporting re...             0\n",
            "1     apparently reassembled from the cutting-room f...             1\n",
            "2     they presume their audience wo n't sit still f...             1\n",
            "3     this is a visually stunning rumination on love...             0\n",
            "4     jonathan parker 's bartleby should have been t...             1\n",
            "...                                                 ...           ...\n",
            "6915  painful , horrifying and oppressively tragic ,...             1\n",
            "6916  take care is nicely performed by a quintet of ...             0\n",
            "6917  the script covers huge , heavy topics in a bla...             1\n",
            "6918  a seriously bad film with seriously warped log...             1\n",
            "6919  a deliciously nonsensical comedy about a city ...             1\n",
            "\n",
            "[6920 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print two reviews from each BERT cluster\n",
        "print(\"\\nTwo reviews from each BERT cluster:\")\n",
        "best_n_clusters = 2\n",
        "for cluster_id in range(best_n_clusters):\n",
        "    cluster_reviews = data[data['bert_cluster'] == cluster_id]['review'][:2]  # Get the first two reviews\n",
        "    print(f\"\\nCluster {cluster_id}:\\n\")\n",
        "    for review in cluster_reviews:\n",
        "        print(review)\n",
        "        print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9Ue7umCS5p-",
        "outputId": "2679ec61-3388-4da9-c928-bc5780dd4270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Two reviews from each BERT cluster:\n",
            "\n",
            "Cluster 0:\n",
            "\n",
            "a stirring , funny and finally transporting re-imagining of beauty and the beast and 1930s horror films\n",
            "\n",
            "this is a visually stunning rumination on love , memory , history and the war between art and commerce .\n",
            "\n",
            "\n",
            "Cluster 1:\n",
            "\n",
            "apparently reassembled from the cutting-room floor of any given daytime soap .\n",
            "\n",
            "they presume their audience wo n't sit still for a sociology lesson , however entertainingly presented , so they trot out the conventional science-fiction elements of bug-eyed monsters and futuristic women in skimpy clothes .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In one paragraph, please compare the results of K-means, DBSCAN, Hierarchical clustering, Word2Vec, and BERT.**"
      ],
      "metadata": {
        "id": "tRijW2aLGONl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Within the range of clustering approaches applied viz., k-means, DBSCAN, Hierarchical Clustering, Word2Vec, and BERT, the distinctive characteristics and resemblances of their outcomes have been observed. The 10 clusters, which varied in size, were the result of k-means method which was the indicator for diverged groups across the data. However, DBSCAN gave us approximately the same number of clusters, but the bigger one consist of noise and some of the others are quite small. The &#039;hierarchical clustering method&#039; does also create a smaller amount of clusters, where one of them taking most of the data points into consideration. Analyzing by WM2 is a mechanism that lead to two logical clusters, with relatively equal sizes, which leads to the idea of different semantic grouping. Correspondingly, BERT clustering identified to clusters which reflected the different semantic content of the same review as well as the distinct sentiment reflected in the underlying of the reviews. While k-means, word2vec, and BERT clustering methods could make out more carefully nuanced differences in the data set, DBSCAN and Hierarchical could hardly find good clusters because of the slightest noise, or possibly because of dependence on parameter settings. Ultimately, each approach gives out a respective view about the structure of the data set that directs us our selection to a suitable clustering technique stated by the nature of the data and expected outcomes.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pIYCj5qyGfSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment.\n",
        "\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "The assignments of this type helped to examine the range of text clustering methods together with the following algorithms: K-means, DBSCAN, Hierarchical, Word2Vec, and BERT. In the end, these mish-mash exercises were a great chance to put into practice what I had learned – going through the dataset of reviews, preprocessing the text data, feature extraction, learning applied clustering, and interpreting the results.\n",
        "\n",
        "\n",
        "Overall, the aspect that stands out the most in regard to the assignment was that there were a variety of clustering methods (that equally carried weaknesses and strengths) used. As an example, K-means clustering has a limitation, however, a simple one that grasps easily the analyst attention, rendering it a popular choice for many use cases. Consequently, from the onset, one ought to specify the number of clusters that have to be deduced without any prerequisite information on the subject. DBSCAN, however, good at discovering clusters of arbitrary patterns and sizes. However, it might be hard to identify high dense data or find cluster which have irregularity within them.\n",
        "\n",
        "\n",
        "The hierarchical clustering helps to come up with all the hierarchical view of the data from a low to high granular level, this helps to explore the relationships. In the meantime, it becomes computationally difficult and noisy as well as error-prone. Word2Vec and BERT embedding techniques have taken the concept of extracting semantics from the text to new and more advanced levels allowing to discover hidden similarities not only between the exact words but also between those that share very deep and subtle meanings. The utilization of these methods is based on the use of pre-trained language models that encode textual information as summary vector representations which in turn allow for effective clustering even in the presence of the minimum number of the labeled data.\n",
        "\n",
        "\n",
        "After this experience, I can clearly perceive the method of topic-based text clustering and compare and contrast different ways how they perform and what results they give. Thus beginning, it would be ideal to consider some more complicated clustering algorithms as well as to measure their efficiency when applied to non-standard datasets, maintaining in this way my text analysis and clustering skills.\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}